---
title: useChat Integration
description: Integrate @built-in-ai/core with the useChat hook in AI SDK v6
---

## Overview

When using this library with the `useChat` hook, you'll need to create a [custom transport](https://ai-sdk.dev/docs/ai-sdk-ui/transport) implementation to handle client-side AI with download progress. 
This is not required, but it makes it easier for you to build better user experiences. 

## Complete Example

```ts
async sendMessages(
  options: {
    chatId: string;
    messages: BuiltInAIUIMessage[];
    abortSignal: AbortSignal | undefined;
  } & {
    trigger: "submit-message" | "submit-tool-result" | "regenerate-message";
    messageId: string | undefined;
  } & ChatRequestOptions,
): Promise<ReadableStream<UIMessageChunk>> {
  const { messages, abortSignal } = options;
  const prompt = await convertToModelMessages(messages);
  const model = builtInAI();

  return createUIMessageStream<BuiltInAIUIMessage>({
    execute: async ({ writer }) => {
      let downloadProgressId: string | undefined;
      const availability = await model.availability();

      // Only track progress if model needs downloading
      if (availability !== "available") {
        await model.createSessionWithProgress((progress: number) => {
          const percent = Math.round(progress * 100);

          if (progress >= 1) {
            if (downloadProgressId) {
              writer.write({
                type: "data-modelDownloadProgress",
                id: downloadProgressId,
                data: {
                  status: "complete",
                  progress: 100,
                  message: "Model finished downloading! Getting ready for inference...",
                },
              });
            }
            return;
          }

          if (!downloadProgressId) {
            downloadProgressId = `download-${Date.now()}`;
          }

          writer.write({
            type: "data-modelDownloadProgress",
            id: downloadProgressId,
            data: {
              status: "downloading",
              progress: percent,
              message: `Downloading browser AI model... ${percent}%`,
            },
            transient: !downloadProgressId, // transient only on first write
          });
        });
      }

      const result = streamText({
        model,
        tools: this.tools,
        stopWhen: stepCountIs(5),
        messages: prompt,
        abortSignal,
        onChunk: (event) => {
          if (event.chunk.type === "text-delta" && downloadProgressId) {
            writer.write({
              type: "data-modelDownloadProgress",
              id: downloadProgressId,
              data: { status: "complete", progress: 100, message: "" },
            });
            downloadProgressId = undefined;
          }
        },
      });

      writer.merge(result.toUIMessageStream({ sendStart: false }));
    },
  });
}
```

### What the Example Includes

- Download progress with UI progress bar and status message updates
- Hybrid client/server architecture with fallback
- Error handling and notifications
- Full integration with `useChat` hook

## Basic Transport Structure

Here's a simplified example of how to structure a custom transport:

```typescript
import { builtInAI, doesBrowserSupportBuiltInAI } from "@built-in-ai/core";
import { streamText } from "ai";
import type { UIMessageStreamWriter } from "ai";

export function createBuiltInAITransport() {
  return async ({ messages, onStream }) => {
    if (!doesBrowserSupportBuiltInAI()) {
      throw new Error("Built-in AI not supported");
    }

    const model = builtInAI();
    const availability = await model.availability();

    if (availability === "downloadable") {
      // Stream download progress to UI
      await model.createSessionWithProgress((progress) => {
        onStream?.writeData({
          type: "modelDownloadProgress",
          status: "downloading",
          progress,
          message: `Downloading model: ${Math.round(progress * 100)}%`,
        });
      });
    }

    const result = await streamText({
      model,
      messages,
    });

    // Stream the response
    for await (const chunk of result.textStream) {
      onStream?.writeText(chunk);
    }
  };
}
```