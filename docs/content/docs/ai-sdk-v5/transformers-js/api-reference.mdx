---
title: API Reference
description: Complete API documentation for @built-in-ai/transformers-js
---

## Provider Functions

### `transformersJS(modelId, settings?)`

Creates a Transformers.js language model instance.

**Parameters:**
- `modelId`: Hugging Face model ID (e.g. `"HuggingFaceTB/SmolLM2-360M-Instruct"`)
- `settings` (optional):
  - `device?: "auto" | "cpu" | "webgpu" | "gpu"` - Inference device (default: "auto")
  - `dtype?: "auto" | "fp32" | "fp16" | "q8" | "q4" | "q4f16"` - Data type for model weights (default: "auto")
  - `isVisionModel?: boolean` - Whether this is a vision model (default: false)
  - `worker?: Worker` - Web Worker for off-main-thread execution
  - `initProgressCallback?: (progress: { progress: number }) => void`
  - `rawInitProgressCallback?: (progress: ProgressInfo) => void`

**Returns:** `TransformersJSLanguageModel`

**Example:**

```typescript
import { transformersJS } from "@built-in-ai/transformers-js";

const model = transformersJS("HuggingFaceTB/SmolLM2-360M-Instruct", {
  device: "webgpu",
  dtype: "q4",
});
```

### `transformersJS.languageModel(modelId, settings?)`

Alias for `transformersJS(modelId, settings?)`.

### `transformersJS.textEmbedding(modelId, settings?)`

Creates an embedding model instance.

**Parameters:**
- `modelId`: Hugging Face embedding model ID (e.g. `"Supabase/gte-small"`)
- `settings` (optional):
  - `device?: "auto" | "cpu" | "webgpu"` - Inference device
  - `dtype?: "auto" | "fp32" | "fp16" | "q8" | "q4" | "q4f16"` - Data type
  - `normalize?: boolean` - Normalize embeddings (default: true)
  - `pooling?: "mean" | "cls" | "max"` - Pooling strategy (default: "mean")
  - `maxTokens?: number` - Maximum input tokens (default: 512)

**Returns:** `TransformersJSEmbeddingModel`

### `transformersJS.transcription(modelId, settings?)`

Creates a transcription model instance.

**Parameters:**
- `modelId`: Whisper model ID (e.g. `"Xenova/whisper-base"`)
- `settings` (optional):
  - `device?: "auto" | "cpu" | "webgpu"` - Inference device
  - `dtype?: "auto" | "fp32" | "fp16" | "q8" | "q4"` - Data type
  - `maxNewTokens?: number` - Maximum tokens (default: 448)
  - `language?: string` - Language hint
  - `returnTimestamps?: boolean` - Return segment timestamps
  - `worker?: Worker` - Web Worker for off-main-thread execution

**Returns:** `TransformersJSTranscriptionModel`

---

## Utility Functions

### `doesBrowserSupportTransformersJS()`

Check if the browser supports Transformers.js with optimal performance.

**Returns:** `boolean` - `true` if WebGPU or WebAssembly is supported

**Example:**

```typescript
import { doesBrowserSupportTransformersJS } from "@built-in-ai/transformers-js";

if (doesBrowserSupportTransformersJS()) {
  // Use client-side inference
} else {
  // Fallback to server-side
}
```

---

## Model Methods

All model types support these methods:

### `availability()`

Checks current availability status.

**Returns:** `Promise<"unavailable" | "downloadable" | "available">`

### `createSessionWithProgress(onProgress?)`

Initialize with progress tracking.

**Parameters:**
- `onProgress?: (p: { progress: number }) => void`

**Returns:** The model instance

**Example:**

```typescript
const model = transformersJS("HuggingFaceTB/SmolLM2-360M-Instruct");

await model.createSessionWithProgress(({ progress }) => {
  console.log(`Download: ${Math.round(progress * 100)}%`);
});
```

---

## Worker Handlers

### `TransformersJSWorkerHandler`

Handler for Web Worker usage with language models.

```typescript title="worker.ts"
import { TransformersJSWorkerHandler } from "@built-in-ai/transformers-js";

const handler = new TransformersJSWorkerHandler();
self.onmessage = (msg: MessageEvent) => handler.onmessage(msg);
```

### `TransformersJSTranscriptionWorkerHandler`

Handler for Web Worker usage with transcription models.

```typescript title="whisper-worker.ts"
import { TransformersJSTranscriptionWorkerHandler } from "@built-in-ai/transformers-js";

const handler = new TransformersJSTranscriptionWorkerHandler();
self.onmessage = (msg: MessageEvent) => handler.onmessage(msg);
```

---

## Types

### `TransformersUIMessage`

Extended UI message type for `useChat` integration:

```typescript
type TransformersUIMessage = UIMessage<
  never,
  {
    modelDownloadProgress: {
      status: "downloading" | "complete" | "error";
      progress?: number;
      message: string;
    };
    notification: {
      message: string;
      level: "info" | "warning" | "error";
    };
  }
>;
```

**Usage:**

```typescript
import type { TransformersUIMessage } from "@built-in-ai/transformers-js";

const { messages } = useChat<TransformersUIMessage>({
  // ...
});
```

