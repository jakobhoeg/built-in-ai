---
title: useChat Integration
description: Integrate @built-in-ai/transformers-js with the useChat hook
---

## Overview

When using this library with the `useChat` hook, you can create hybrid applications that seamlessly switch between client-side and server-side inference.

## TransformersUIMessage Type

Import `TransformersUIMessage` from `@built-in-ai/transformers-js` for proper typing with data parts:

```typescript
import type { TransformersUIMessage } from "@built-in-ai/transformers-js";

// TransformersUIMessage extends UIMessage with custom data parts:
// - modelDownloadProgress: { status, progress?, message }
// - notification: { message, level }
```

### Type Definition

```typescript
type TransformersUIMessage = UIMessage<
  never,
  {
    modelDownloadProgress: {
      status: "downloading" | "complete" | "error";
      progress?: number;
      message: string;
    };
    notification: {
      message: string;
      level: "info" | "warning" | "error";
    };
  }
>;
```

## Data Parts

### modelDownloadProgress

Tracks model download status and progress:

- `status`: Current download state (`"downloading"`, `"complete"`, or `"error"`)
- `progress`: Optional number from 0-1 indicating download progress
- `message`: Human-readable status message

### notification

Displays temporary messages and alerts to users:

- `message`: The notification text
- `level`: Severity level (`"info"`, `"warning"`, or `"error"`)

## Complete Example

See the complete working example in the repository at `examples/next-hybrid/` which includes:

- **Automatic fallback**: Client-side when supported, server-side otherwise
- **Download progress**: Real-time progress tracking for model downloads
- **Error handling**: Graceful error handling and notifications
- **Full integration**: Complete integration with `useChat` hook

## Basic Implementation

```typescript
import { useChat } from "ai/react";
import { transformersJS, TransformersUIMessage } from "@built-in-ai/transformers-js";

// Create the model instance
const model = transformersJS("HuggingFaceTB/SmolLM2-360M-Instruct", {
  device: "webgpu",
  dtype: "q4",
  worker: new Worker(new URL("./worker.ts", import.meta.url), {
    type: "module",
  }),
});

function ChatComponent() {
  const { messages, input, handleInputChange, handleSubmit, status } =
    useChat<TransformersUIMessage>({
      // Your custom transport implementation
    });

  return (
    <div>
      {messages.map((message) => (
        <div key={message.id}>
          <strong>{message.role}:</strong>
          {message.parts?.map((part, i) => {
            if (part.type === "text") {
              return <span key={i}>{part.text}</span>;
            }
            // Handle data parts for progress, etc.
            return null;
          })}
        </div>
      ))}
      
      <form onSubmit={handleSubmit}>
        <input value={input} onChange={handleInputChange} />
        <button type="submit">Send</button>
      </form>
    </div>
  );
}
```

## Displaying Download Progress

```typescript
function ChatComponent() {
  const { messages } = useChat<TransformersUIMessage>({
    // ...
  });

  // Find download progress from messages
  const downloadProgress = messages
    .flatMap(m => m.parts || [])
    .find(p => p.type === "data" && p.data?.type === "modelDownloadProgress");

  return (
    <div>
      {downloadProgress && (
        <div className="progress-bar">
          <div 
            className="progress-fill"
            style={{ width: `${(downloadProgress.data.progress || 0) * 100}%` }}
          />
          <span>{downloadProgress.data.message}</span>
        </div>
      )}
      
      {/* Rest of your chat UI */}
    </div>
  );
}
```

<Callout type="info">
  For a production-ready implementation, see the complete example in the repository which handles edge cases, errors, and provides a polished UI.
</Callout>

