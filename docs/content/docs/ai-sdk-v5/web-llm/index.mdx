---
title: "@built-in-ai/web-llm"
description: WebLLM provider for Vercel AI SDK v5 - High-performance in-browser LLM inference
---

High-performance in-browser LLM inference using [WebLLM](https://github.com/mlc-ai/web-llm) with WebGPU acceleration. Run popular open-source models directly in your web browser.

<Callout type="info">
  This library is still in early development. Updates might come quite frequently.
</Callout>

## Installation

```bash
npm i @built-in-ai/web-llm@^0.3.2 @mlc-ai/web-llm ai@^5
```

## Browser Requirements

A WebGPU-compatible browser is required. Check [WebGPU browser support](https://caniuse.com/webgpu) for compatibility information.

<Callout type="warn">
  WebGPU is required for WebLLM. Browsers without WebGPU support will not work.
</Callout>

## Quick Start

### Basic Usage

```typescript
import { streamText } from "ai";
import { webLLM } from "@built-in-ai/web-llm";

const result = streamText({
  model: webLLM("Llama-3.2-3B-Instruct-q4f16_1-MLC"),
  messages: [{ role: "user", content: "Hello, how are you?" }],
});

for await (const chunk of result.textStream) {
  console.log(chunk);
}
```

## Web Worker Usage

For optimal performance, offload model computation to a Web Worker:

### 1. Create worker.ts

```typescript
import { WebWorkerMLCEngineHandler } from "@built-in-ai/web-llm";

const handler = new WebWorkerMLCEngineHandler();
self.onmessage = (msg: MessageEvent) => {
  handler.onmessage(msg);
};
```

### 2. Use the Worker

```typescript
import { streamText } from "ai";
import { webLLM } from "@built-in-ai/web-llm";

const result = streamText({
  model: webLLM("Qwen3-0.6B-q0f16-MLC", {
    worker: new Worker(new URL("./worker.ts", import.meta.url), {
      type: "module",
    }),
  }),
  messages: [{ role: "user", content: "Hello, how are you?" }],
});

for await (const chunk of result.textStream) {
  console.log(chunk);
}
```

## Download Progress Tracking

When using models for the first time, track download progress to improve UX:

```typescript
import { streamText } from "ai";
import { webLLM } from "@built-in-ai/web-llm";

const model = webLLM("Llama-3.2-3B-Instruct-q4f16_1-MLC");
const availability = await model.availability();

if (availability === "unavailable") {
  console.log("Browser doesn't support WebLLM");
  return;
}

if (availability === "downloadable") {
  await model.createSessionWithProgress((progress) => {
    console.log(`Download progress: ${Math.round(progress.progress * 100)}%`);
    console.log(progress.text);
  });
}

// Model is ready
const result = streamText({
  model,
  messages: [{ role: "user", content: "Hello!" }],
});
```

## Tool Calling

<Callout type="warn">
  Some models might struggle with tool calling. For best results, use a reasoning model like Qwen3.
</Callout>

```typescript
import { streamText, stepCountIs, tool } from "ai";
import { webLLM } from "@built-in-ai/web-llm";
import { z } from "zod";

const result = streamText({
  model: webLLM("Qwen3-1.7B-q4f16_1-MLC"),
  tools: {
    weather: tool({
      description: "Get the weather in a location",
      parameters: z.object({
        location: z.string().describe("The location to get the weather for"),
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
      }),
    }),
  },
  stopWhen: stepCountIs(5),
  prompt: "What is the weather in San Francisco?",
});
```

When using with `useChat`, set `sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls`.

## Integration with useChat

Create a custom transport implementation for client-side AI with download progress. Import `WebLLMUIMessage` for proper typing:

```typescript
import type { WebLLMUIMessage } from "@built-in-ai/web-llm";

// WebLLMUIMessage extends UIMessage with custom data parts:
// - modelDownloadProgress: { status, progress?, message }
// - notification: { message, level }
```

See the [examples/next-hybrid/app/web-llm](https://github.com/jakobhoeg/built-in-ai/tree/main/examples/next-hybrid/app/web-llm) directory for a complete working example.

## API Reference

### `webLLM(modelId, settings?)`

Creates a WebLLM model instance.

**Parameters:**
- `modelId`: Model identifier from the [supported models list](https://github.com/mlc-ai/web-llm/blob/main/src/config.ts)
- `settings` (optional):
  - `appConfig?: AppConfig` - Custom app configuration
  - `initProgressCallback?: (progress: WebLLMProgress) => void` - Progress callback
  - `engineConfig?: MLCEngineConfig` - Engine configuration
  - `worker?: Worker` - Web Worker for better performance

**Returns:** `WebLLMLanguageModel`

### `doesBrowserSupportWebLLM()`

Check if the browser supports WebLLM (WebGPU).

```typescript
import { doesBrowserSupportWebLLM } from "@built-in-ai/web-llm";

if (doesBrowserSupportWebLLM()) {
  // Use WebLLM
} else {
  // Fallback to server-side
}
```

### `WebLLMLanguageModel.availability()`

Check the current availability status.

**Returns:** `Promise<"unavailable" | "downloadable" | "downloading" | "available">`

### `WebLLMLanguageModel.createSessionWithProgress(onProgress?)`

Create a session with download progress monitoring.

**Parameters:**
- `onDownloadProgress?: (progress: WebLLMProgress) => void`

**Returns:** `Promise<MLCEngineInterface>`

### `WebLLMProgress`

Progress report type during model initialization:

```typescript
interface WebLLMProgress {
  progress: number; // 0-1
  timeElapsed: number; // in ms
  text: string; // progress text
}
```

### `WebLLMUIMessage`

Extended UI message type for `useChat` integration:

```typescript
type WebLLMUIMessage = UIMessage<
  never,
  {
    modelDownloadProgress: {
      status: "downloading" | "complete" | "error";
      progress?: number;
      message: string;
    };
    notification: {
      message: string;
      level: "info" | "warning" | "error";
    };
  }
>;
```

### `WebWorkerMLCEngineHandler`

Handler for Web Worker usage:

```typescript
import { WebWorkerMLCEngineHandler } from "@built-in-ai/web-llm";

const handler = new WebWorkerMLCEngineHandler();
self.onmessage = (msg: MessageEvent) => handler.onmessage(msg);
```

## Supported Models

WebLLM supports a wide range of models. Here are some popular choices:

| Model | Size | Use Case |
|-------|------|----------|
| `Qwen3-0.6B-q0f16-MLC` | ~600M | Small, fast |
| `Qwen3-1.7B-q4f16_1-MLC` | ~1.7B | Reasoning |
| `Llama-3.2-1B-Instruct-q4f16_1-MLC` | ~1B | General |
| `Llama-3.2-3B-Instruct-q4f16_1-MLC` | ~3B | General |
| `Phi-3.5-mini-instruct-q4f16_1-MLC` | ~3.8B | General |
| `gemma-2-2b-it-q4f16_1-MLC` | ~2B | General |

See the [complete list of supported models](https://github.com/mlc-ai/web-llm/blob/main/src/config.ts) in the WebLLM repository.

## Performance Tips

1. **Use Web Workers**: Always use a Web Worker for production to avoid blocking the main thread
2. **Choose appropriate model sizes**: Smaller models (1-3B) work better in browsers
3. **Enable quantization**: Use q4f16 or q8 quantized models for faster inference
4. **Check WebGPU support**: Verify WebGPU is available before loading models

